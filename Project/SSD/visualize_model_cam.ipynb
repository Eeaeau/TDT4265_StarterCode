{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.getcwd())) # Include ../SSD in path\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from vizer.draw import draw_boxes\n",
    "from tops.config import instantiate, LazyConfig\n",
    "from ssd import utils\n",
    "np.random.seed(0)\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.transforms.functional import normalize, resize, to_pil_image\n",
    "from torchvision.models import resnet34\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "import torch.functional as F\n",
    "import requests\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssd.modeling.retinanetOutputWrapper import RetinaNetOutputWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"configs/tdt4265_fpn.py\"\n",
    "# cfg = LazyConfig.load(config_path)\n",
    "\n",
    "# Get your input\n",
    "img = read_image(\"data/tdt4265_2022/images/train/trip007_glos_Video00000_3.png\")\n",
    "# plt.imshow(to_pil_image(img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving SSD outputs to: outputs/\n",
      "Model used: resnet34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\sebsk/.cache\\torch\\hub\\pytorch_vision_v0.11.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------- [64, 128, 256, 512, 1024, 1024] ---------------------------------------\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 16:35:23,729 [INFO ] Loaded checkpoint from outputs\\tdt4265_fpn\\checkpoints\\51.ckpt\n"
     ]
    }
   ],
   "source": [
    "from performance_assessment.save_comparison_images import get_config, get_trained_model, get_dataloader\n",
    "\n",
    "cfg = get_config(config_path)\n",
    "model = get_trained_model(cfg)\n",
    "\n",
    "# # x = normalize(img, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "# x = torch.randn(1, 3, 128, 1024).to(\"cuda\")\n",
    "\n",
    "# model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch-grad-cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "from pytorch_grad_cam import GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "wrapped_model = RetinaNetOutputWrapper(model).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128, 1024])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_tensor = normalize(img.float()/255, mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "print(input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    wrapped_model = wrapped_model.cuda()\n",
    "    input_tensor = input_tensor.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_tensor.unsqueeze(0).shape)\n",
    "output = wrapped_model(input_tensor.unsqueeze(0))\n",
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_np = output.cpu().detach().numpy()\n",
    "print(output_np.shape)\n",
    "\n",
    "plt.rc('figure', dpi=400)\n",
    "plt.imshow(output_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer = [wrapped_model.model.feature_extractor.fpn._modules]\n",
    "print(target_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer = [model.feature_extractor.fpn._modules]\n",
    "# print(target_layer)\n",
    "print(target_layer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layers = [model.model.backbone.layer4]\n",
    "targets = [SemanticSegmentationTarget(car_category, car_mask_float)]\n",
    "with GradCAM(model=model,\n",
    "             target_layers=target_layers,\n",
    "             use_cuda=torch.cuda.is_available()) as cam:\n",
    "    grayscale_cam = cam(input_tensor=input_tensor,\n",
    "                        targets=targets)[0, :]\n",
    "    cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "Image.fromarray(cam_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "image_url = \"https://farm1.staticflickr.com/6/9606553_ccc7518589_z.jpg\"\n",
    "image = np.array(Image.open(requests.get(image_url, stream=True).raw))\n",
    "rgb_img = np.float32(image) / 255\n",
    "input_tensor = preprocess_image(rgb_img,\n",
    "                                mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "# Taken from the torchvision tutorial\n",
    "# https://pytorch.org/vision/stable/auto_examples/plot_visualization_utils.html\n",
    "model = deeplabv3_resnet50(pretrained=True, progress=False)\n",
    "model = model.eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    input_tensor = input_tensor.cuda()\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(type(output), output.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModelOutputWrapper(torch.nn.Module):\n",
    "    def __init__(self, model): \n",
    "        super(SegmentationModelOutputWrapper, self).__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)[\"out\"]\n",
    "    \n",
    "model = SegmentationModelOutputWrapper(model)\n",
    "output = model(input_tensor)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_masks = torch.nn.functional.softmax(output, dim=1).cpu()\n",
    "sem_classes = [\n",
    "    '__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "]\n",
    "sem_class_to_idx = {cls: idx for (idx, cls) in enumerate(sem_classes)}\n",
    "\n",
    "car_category = sem_class_to_idx[\"car\"]\n",
    "car_mask = normalized_masks[0, :, :, :].argmax(axis=0).detach().cpu().numpy()\n",
    "car_mask_uint8 = 255 * np.uint8(car_mask == car_category)\n",
    "car_mask_float = np.float32(car_mask == car_category)\n",
    "\n",
    "both_images = np.hstack((image, np.repeat(car_mask_uint8[:, :, None], 3, axis=-1)))\n",
    "Image.fromarray(both_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM\n",
    "\n",
    "class SemanticSegmentationTarget:\n",
    "    def __init__(self, category, mask):\n",
    "        self.category = category\n",
    "        self.mask = torch.from_numpy(mask)\n",
    "        if torch.cuda.is_available():\n",
    "            self.mask = self.mask.cuda()\n",
    "        \n",
    "    def __call__(self, model_output):\n",
    "        return (model_output[self.category, :, : ] * self.mask).sum()\n",
    "\n",
    "    \n",
    "target_layers = [model.model.backbone.layer4]\n",
    "targets = [SemanticSegmentationTarget(car_category, car_mask_float)]\n",
    "with GradCAM(model=model,\n",
    "             target_layers=target_layers,\n",
    "             use_cuda=torch.cuda.is_available()) as cam:\n",
    "    grayscale_cam = cam(input_tensor=input_tensor,\n",
    "                        targets=targets)[0, :]\n",
    "    cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "Image.fromarray(cam_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchcam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchcam.methods import SmoothGradCAMpp\n",
    "\n",
    "\n",
    "\n",
    "model_resnet = resnet34(pretrained=True).eval()\n",
    "print(model_resnet)\n",
    "# model = cfg.model\n",
    "# cam_extractor = SmoothGradCAMpp(model, target_layer=target_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Preprocess it for your chosen model\n",
    "input_tensor = normalize(resize(img, (224, 224)) / 255., [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "# Preprocess your data and feed it to the model\n",
    "out = model(input_tensor.unsqueeze(0))\n",
    "# Retrieve the CAM by passing the class index and the model output\n",
    "activation_map = cam_extractor(out.squeeze(0).argmax().item(), out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Visualize the raw CAM\n",
    "plt.imshow(activation_map[0].squeeze(0).numpy()); plt.axis('off'); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchcam.utils import overlay_mask\n",
    "\n",
    "# Resize the CAM and overlay it\n",
    "result = overlay_mask(to_pil_image(img), to_pil_image(activation_map[0].squeeze(0), mode='F'), alpha=0.5)\n",
    "\n",
    "plt.rc('figure', dpi=300)\n",
    "# Display it\n",
    "plt.imshow(result); plt.axis('off'); plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "088b827b8b985f163c2bc9e7571c109fd1cd09e7d4200c98bc68a07b57088618"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
