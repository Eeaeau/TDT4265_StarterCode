{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.getcwd())) # Include ../SSD in path\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from vizer.draw import draw_boxes\n",
    "from tops.config import instantiate, LazyConfig\n",
    "from ssd import utils\n",
    "np.random.seed(0)\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.transforms.functional import normalize, resize, to_pil_image\n",
    "from torchvision.models import resnet34\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "import torch.functional as F\n",
    "import requests\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssd.modeling.retinanetOutputWrapper import RetinaNetOutputWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"configs/tdt4265_fpn.py\"\n",
    "# cfg = LazyConfig.load(config_path)\n",
    "\n",
    "# Get your input\n",
    "img = read_image(\"data/tdt4265_2022/images/train/trip007_glos_Video00000_3.png\")\n",
    "# plt.imshow(to_pil_image(img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving SSD outputs to: outputs/\n",
      "Model used: resnet34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\sebsk/.cache\\torch\\hub\\pytorch_vision_v0.11.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------- [64, 128, 256, 512, 1024, 1024] ---------------------------------------\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 18:20:01,112 [INFO ] Loaded checkpoint from outputs\\tdt4265_fpn\\checkpoints\\51.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SSD300(\n",
       "  (feature_extractor): ResnetWithFPN(\n",
       "    (body): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Module(\n",
       "        (0): Module(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): Module(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): Module(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Module(\n",
       "        (0): Module(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Module(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Module(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): Module(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): Module(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Module(\n",
       "        (0): Module(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Module(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Module(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): Module(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): Module(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): Module(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): Module(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Module(\n",
       "        (0): Module(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Module(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Module(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): Module(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (extras): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2))\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(2, 2))\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (5): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (loss_func): SSDMultiboxLoss(\n",
       "    (sl1_loss): SmoothL1Loss()\n",
       "  )\n",
       "  (regression_heads): ModuleList(\n",
       "    (0): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (2): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (classification_heads): ModuleList(\n",
       "    (0): Conv2d(256, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Conv2d(256, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (2): Conv2d(256, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): Conv2d(256, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from performance_assessment.save_comparison_images import get_config, get_trained_model, get_dataloader\n",
    "\n",
    "cfg = get_config(config_path)\n",
    "model = get_trained_model(cfg)\n",
    "\n",
    "# # x = normalize(img, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "# x = torch.randn(1, 3, 128, 1024).to(\"cuda\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch-grad-cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "from pytorch_grad_cam import GradCAM, EigenCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "\n",
    "wrapped_model = RetinaNetOutputWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128, 1024])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_tensor = normalize(img.float()/255, mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "print(input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    wrapped_model = wrapped_model.cuda()\n",
    "    input_tensor = input_tensor.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(input_tensor.unsqueeze(0).shape)\n",
    "# output = wrapped_model(input_tensor.unsqueeze(0))\n",
    "# print(type(output))\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_transform(x):\n",
    "    print(x)\n",
    "    target_size = x[1].size()[-2 : ]\n",
    "    activations = []\n",
    "    for key, value in x.items():\n",
    "        activations.append(torch.nn.functional.interpolate(torch.abs(value), target_size, mode='bilinear'))\n",
    "    activations = torch.cat(activations, axis=1)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'FeaturePyramidNetwork' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sebsk\\Documents\\Git\\TDT4265_StarterCode\\Project\\SSD\\visualize_model_cam.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sebsk/Documents/Git/TDT4265_StarterCode/Project/SSD/visualize_model_cam.ipynb#ch0000024?line=0'>1</a>\u001b[0m cam \u001b[39m=\u001b[39m EigenCAM(wrapped_model, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sebsk/Documents/Git/TDT4265_StarterCode/Project/SSD/visualize_model_cam.ipynb#ch0000024?line=1'>2</a>\u001b[0m     target_layers\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfeature_extractor\u001b[39m.\u001b[39;49mfpn, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sebsk/Documents/Git/TDT4265_StarterCode/Project/SSD/visualize_model_cam.ipynb#ch0000024?line=2'>3</a>\u001b[0m     use_cuda\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mis_available(), \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sebsk/Documents/Git/TDT4265_StarterCode/Project/SSD/visualize_model_cam.ipynb#ch0000024?line=3'>4</a>\u001b[0m     reshape_transform\u001b[39m=\u001b[39;49mreshape_transform)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sebsk/Documents/Git/TDT4265_StarterCode/Project/SSD/visualize_model_cam.ipynb#ch0000024?line=5'>6</a>\u001b[0m cam\u001b[39m.\u001b[39muses_gradients\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\SSD\\lib\\site-packages\\pytorch_grad_cam\\eigen_cam.py:10\u001b[0m, in \u001b[0;36mEigenCAM.__init__\u001b[1;34m(self, model, target_layers, use_cuda, reshape_transform)\u001b[0m\n\u001b[0;32m      <a href='file:///c%3A/ProgramData/Miniconda3/envs/SSD/lib/site-packages/pytorch_grad_cam/eigen_cam.py?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model, target_layers, use_cuda\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m      <a href='file:///c%3A/ProgramData/Miniconda3/envs/SSD/lib/site-packages/pytorch_grad_cam/eigen_cam.py?line=8'>9</a>\u001b[0m              reshape_transform\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> <a href='file:///c%3A/ProgramData/Miniconda3/envs/SSD/lib/site-packages/pytorch_grad_cam/eigen_cam.py?line=9'>10</a>\u001b[0m     \u001b[39msuper\u001b[39;49m(EigenCAM, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(model, target_layers, use_cuda,\n\u001b[0;32m     <a href='file:///c%3A/ProgramData/Miniconda3/envs/SSD/lib/site-packages/pytorch_grad_cam/eigen_cam.py?line=10'>11</a>\u001b[0m                                    reshape_transform)\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\SSD\\lib\\site-packages\\pytorch_grad_cam\\base_cam.py:27\u001b[0m, in \u001b[0;36mBaseCAM.__init__\u001b[1;34m(self, model, target_layers, use_cuda, reshape_transform, compute_input_gradient, uses_gradients)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/ProgramData/Miniconda3/envs/SSD/lib/site-packages/pytorch_grad_cam/base_cam.py?line=24'>25</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_input_gradient \u001b[39m=\u001b[39m compute_input_gradient\n\u001b[0;32m     <a href='file:///c%3A/ProgramData/Miniconda3/envs/SSD/lib/site-packages/pytorch_grad_cam/base_cam.py?line=25'>26</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muses_gradients \u001b[39m=\u001b[39m uses_gradients\n\u001b[1;32m---> <a href='file:///c%3A/ProgramData/Miniconda3/envs/SSD/lib/site-packages/pytorch_grad_cam/base_cam.py?line=26'>27</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivations_and_grads \u001b[39m=\u001b[39m ActivationsAndGradients(\n\u001b[0;32m     <a href='file:///c%3A/ProgramData/Miniconda3/envs/SSD/lib/site-packages/pytorch_grad_cam/base_cam.py?line=27'>28</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, target_layers, reshape_transform)\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\SSD\\lib\\site-packages\\pytorch_grad_cam\\activations_and_gradients.py:11\u001b[0m, in \u001b[0;36mActivationsAndGradients.__init__\u001b[1;34m(self, model, target_layers, reshape_transform)\u001b[0m\n\u001b[0;32m      <a href='file:///c%3A/ProgramData/Miniconda3/envs/SSD/lib/site-packages/pytorch_grad_cam/activations_and_gradients.py?line=8'>9</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreshape_transform \u001b[39m=\u001b[39m reshape_transform\n\u001b[0;32m     <a href='file:///c%3A/ProgramData/Miniconda3/envs/SSD/lib/site-packages/pytorch_grad_cam/activations_and_gradients.py?line=9'>10</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m []\n\u001b[1;32m---> <a href='file:///c%3A/ProgramData/Miniconda3/envs/SSD/lib/site-packages/pytorch_grad_cam/activations_and_gradients.py?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m target_layer \u001b[39min\u001b[39;00m target_layers:\n\u001b[0;32m     <a href='file:///c%3A/ProgramData/Miniconda3/envs/SSD/lib/site-packages/pytorch_grad_cam/activations_and_gradients.py?line=11'>12</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mappend(\n\u001b[0;32m     <a href='file:///c%3A/ProgramData/Miniconda3/envs/SSD/lib/site-packages/pytorch_grad_cam/activations_and_gradients.py?line=12'>13</a>\u001b[0m         target_layer\u001b[39m.\u001b[39mregister_forward_hook(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_activation))\n\u001b[0;32m     <a href='file:///c%3A/ProgramData/Miniconda3/envs/SSD/lib/site-packages/pytorch_grad_cam/activations_and_gradients.py?line=13'>14</a>\u001b[0m     \u001b[39m# Because of https://github.com/pytorch/pytorch/issues/61519,\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/ProgramData/Miniconda3/envs/SSD/lib/site-packages/pytorch_grad_cam/activations_and_gradients.py?line=14'>15</a>\u001b[0m     \u001b[39m# we don't use backward hook to record gradients.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'FeaturePyramidNetwork' object is not iterable"
     ]
    }
   ],
   "source": [
    "cam = EigenCAM(wrapped_model, \n",
    "    target_layers=model.feature_extractor.fpn, \n",
    "    use_cuda=torch.cuda.is_available(), \n",
    "    reshape_transform=reshape_transform)\n",
    "\n",
    "cam.uses_gradients=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sebsk\\Documents\\Git\\TDT4265_StarterCode\\Project\\SSD\\visualize_model_cam.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sebsk/Documents/Git/TDT4265_StarterCode/Project/SSD/visualize_model_cam.ipynb#ch0000010?line=0'>1</a>\u001b[0m output_np \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sebsk/Documents/Git/TDT4265_StarterCode/Project/SSD/visualize_model_cam.ipynb#ch0000010?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(output_np\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sebsk/Documents/Git/TDT4265_StarterCode/Project/SSD/visualize_model_cam.ipynb#ch0000010?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mrc(\u001b[39m'\u001b[39m\u001b[39mfigure\u001b[39m\u001b[39m'\u001b[39m, dpi\u001b[39m=\u001b[39m\u001b[39m400\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "\n",
    "output_np = output.cpu().detach().numpy()\n",
    "print(output_np.shape)\n",
    "\n",
    "plt.rc('figure', dpi=400)\n",
    "plt.imshow(output_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OrderedDict([('inner_blocks', ModuleList(\n",
      "  (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (5): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      ")), ('layer_blocks', ModuleList(\n",
      "  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "))])]\n"
     ]
    }
   ],
   "source": [
    "target_layer = [wrapped_model.model.feature_extractor.fpn._modules]\n",
    "print(target_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer = [model.feature_extractor.fpn._modules]\n",
    "# print(target_layer)\n",
    "print(target_layer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layers = [model.model.backbone.layer4]\n",
    "targets = [SemanticSegmentationTarget(car_category, car_mask_float)]\n",
    "with GradCAM(model=model,\n",
    "             target_layers=target_layers,\n",
    "             use_cuda=torch.cuda.is_available()) as cam:\n",
    "    grayscale_cam = cam(input_tensor=input_tensor,\n",
    "                        targets=targets)[0, :]\n",
    "    cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "Image.fromarray(cam_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "image_url = \"https://farm1.staticflickr.com/6/9606553_ccc7518589_z.jpg\"\n",
    "image = np.array(Image.open(requests.get(image_url, stream=True).raw))\n",
    "rgb_img = np.float32(image) / 255\n",
    "input_tensor = preprocess_image(rgb_img,\n",
    "                                mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "# Taken from the torchvision tutorial\n",
    "# https://pytorch.org/vision/stable/auto_examples/plot_visualization_utils.html\n",
    "model = deeplabv3_resnet50(pretrained=True, progress=False)\n",
    "model = model.eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    input_tensor = input_tensor.cuda()\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(type(output), output.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModelOutputWrapper(torch.nn.Module):\n",
    "    def __init__(self, model): \n",
    "        super(SegmentationModelOutputWrapper, self).__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)[\"out\"]\n",
    "    \n",
    "model = SegmentationModelOutputWrapper(model)\n",
    "output = model(input_tensor)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_masks = torch.nn.functional.softmax(output, dim=1).cpu()\n",
    "sem_classes = [\n",
    "    '__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "]\n",
    "sem_class_to_idx = {cls: idx for (idx, cls) in enumerate(sem_classes)}\n",
    "\n",
    "car_category = sem_class_to_idx[\"car\"]\n",
    "car_mask = normalized_masks[0, :, :, :].argmax(axis=0).detach().cpu().numpy()\n",
    "car_mask_uint8 = 255 * np.uint8(car_mask == car_category)\n",
    "car_mask_float = np.float32(car_mask == car_category)\n",
    "\n",
    "both_images = np.hstack((image, np.repeat(car_mask_uint8[:, :, None], 3, axis=-1)))\n",
    "Image.fromarray(both_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM\n",
    "\n",
    "class SemanticSegmentationTarget:\n",
    "    def __init__(self, category, mask):\n",
    "        self.category = category\n",
    "        self.mask = torch.from_numpy(mask)\n",
    "        if torch.cuda.is_available():\n",
    "            self.mask = self.mask.cuda()\n",
    "        \n",
    "    def __call__(self, model_output):\n",
    "        return (model_output[self.category, :, : ] * self.mask).sum()\n",
    "\n",
    "    \n",
    "target_layers = [model.model.backbone.layer4]\n",
    "targets = [SemanticSegmentationTarget(car_category, car_mask_float)]\n",
    "with GradCAM(model=model,\n",
    "             target_layers=target_layers,\n",
    "             use_cuda=torch.cuda.is_available()) as cam:\n",
    "    grayscale_cam = cam(input_tensor=input_tensor,\n",
    "                        targets=targets)[0, :]\n",
    "    cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "Image.fromarray(cam_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchcam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchcam.methods import SmoothGradCAMpp\n",
    "\n",
    "\n",
    "\n",
    "model_resnet = resnet34(pretrained=True).eval()\n",
    "print(model_resnet)\n",
    "# model = cfg.model\n",
    "# cam_extractor = SmoothGradCAMpp(model, target_layer=target_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Preprocess it for your chosen model\n",
    "input_tensor = normalize(resize(img, (224, 224)) / 255., [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "# Preprocess your data and feed it to the model\n",
    "out = model(input_tensor.unsqueeze(0))\n",
    "# Retrieve the CAM by passing the class index and the model output\n",
    "activation_map = cam_extractor(out.squeeze(0).argmax().item(), out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Visualize the raw CAM\n",
    "plt.imshow(activation_map[0].squeeze(0).numpy()); plt.axis('off'); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchcam.utils import overlay_mask\n",
    "\n",
    "# Resize the CAM and overlay it\n",
    "result = overlay_mask(to_pil_image(img), to_pil_image(activation_map[0].squeeze(0), mode='F'), alpha=0.5)\n",
    "\n",
    "plt.rc('figure', dpi=300)\n",
    "# Display it\n",
    "plt.imshow(result); plt.axis('off'); plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "088b827b8b985f163c2bc9e7571c109fd1cd09e7d4200c98bc68a07b57088618"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
